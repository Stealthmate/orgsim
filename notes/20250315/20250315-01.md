(d = 00:13 + 00:70)

Phew, I think I added a lot of stuff, so let's go through it piece of piece.

First of all, I was trying to implement a specific model (and I kinda almost finished it), but during that process I found some problems with `world`, so I'll be comitting the fixes for those first.

##### Metrics

I added a simple ABC definition for `Metrics` inside `world`, and wrote the logic for logging the most fundamental values for each model. Currently those are mostly related to the number of individuals in the world - number of people alive, number of people killed and number of people recruited. Though I should be able to add more metrics pretty easily, if I ever need to.

##### Other stuff

- Individuals' `init` method was not being called on recruitment, so I fixed that.
- Individuals should be able to do cleanup on death, so I added a `die` method to hold that logic.
- `BaseWorldState` holds the seeding logic as a classmethod now.
---

`(d = 00:28)`

Again, by trying to implement a specific model I stumbled upon problems with the `world` abstraction.

- Turns out I hadn't implemented initialization logic for `IndividualState`. I had forgotten that I'm storing the `Individual` (implementation) and `IndividualState` separately. I just abstracted it so that I always set/unset both at the same time.
- I added a new method to `Nature` for setting the generating the initial individuals in the world.
---

`(d = 00:25)`

I want to implement some basic plotting logic in order ot be able to compare different worlds. I'll try to do that under `explore`.
---

`(d = 00:40)`

I moved `base_state` inside the `state` variable. Nothing too fancy.
---

`(d = 00:06)`

I implemented some simple functions to plot metrics for the `World` itself. This should give me a reasonable common way of comparing different worlds.
---

`(d = 00:15)`

Ok, this is a gonna be a big one.

I moved all the new implementations into `orgism.world.v1`. I thought about it for a while, and I think what I'm trying to do is set up a way to preserve older code without having to do commit magic (going back to previous commits) just so that my experiments can run.

Then again....

I'm starting to think that I should keep my notes in a separate repository, and then reference specific versions of the main codebase as needed in my experiments. Or maybe I can keep everything in a monorepo, but put the notes in a different environment, then reference the main package from tagged github versions as needed. Yeah, that should work as well. For increased readibility, I could split up all my commits and give them a specific prefix depending on content. That is to say, for example I could use `model: ` for commits changing the model and `journal: ` for commits regarding my notes/experiments.

Back to the model itself. I'm commiting the example model I was working on while improving the world and metrics functions. It's still not complete, but it's complete enough to be able to produce a population graph. Next I should work on implementing the remaining functions and making sure that it works as intended.

While doing that, I'd like to also work on producing some kind of "final" article regarding some specific org setup. E.g. I could take all the selfishness stuff I was playing around with, and write an article about how the org performs using different parameter values. It won't be perfect by any means, but it will give me some specific constraints to work on, which should make thinking about this whole concept easier.

Anyway, as usual I have a lot left to do.
---

Versioning is confusing...

So I think I might actually want to have multiple version chains for different things.

1. The "library"
2. My notes
3. Each "paper"

The first two are pretty self-explanatory. As for the "paper" part - I think I should set myself a specific goal before I start doing any kind of work. Then I can dedicate 1 version chain to that goal, and do work in small increments in order to achieve it. The key idea here is that each "goal" can only ever use 1 version of the library. On the other hand, the library should move fairly quickly, and it shoud aim to become more and more expressive, that is to say it should become easier and eaiser to express differen concepts within the same version of the library.

So now the question becomes - **what is my current goal? What do I aim to achieve with this so-called "research"?**

Originally, my goal would have been to construct a model that is more or less equivalent to my current company, construct a few different player strategies, and then find a startegy that is *optimal*. Optimality here would be a state where the most selfish acts are simultaneously the most selfless, that is to say, they help the Org as well as the Individuals.

Needless to say, this is a pretty big goal. After working on it for a while, I've come te realize that I won't be able to complete this in one go. So now I need to split it into smaller ones. What is some specific goal that is small enough for me to tackle alone, yet specific and meaningful enough to guide me towards progress to the "big" goal?

I already have a basic understanding of how I want to model my experiments. I have a farily sophisticated implementation of a "game engine" (the `World`) which can run orgs with different rules. The next step here would be to find a way to compare different models. But what exactly am I trying to compare?

I think there are two fundamental sets of rules that I need to be thinking about. The first is the rules of the world itself - the rules governing what the world is, how the Org and the Individuals act inside it, etc. Those are rules that I don't want to vary between experiments. Rules that are intended to be constant, and as faithful as possible to the real world. On the other hand, as I do more and more experiments, I will most likely find ways to refine those rules. In that sense, they do change over time.

The second set of rules is the one governing the specific actions of each entity inside the world. Those are the rules that govern what it means for Individuals to act, how the Org evaluates the Individuals and replaces them, and so on. Those are rules that I do want to vary between experiments, for the purposes of finding an optimal result.

And here I am again, talking about "optimality". What exactly do I mean by that?

One basic idea, that I can think of right now, is the population of the world. One view on optimality would imply that the Org survives indefinitely. But I also need to consider optimality for the individual. I guess what I'm trying to do, specifically, is to prove that certain conditions produce a maxiumal combination of both Org and Individual optimality criteria. A more mathematical way of approaching this would be to define this whole idea as a problem of dual value optimization.

I tried googling this and stumbled upon a few different ideas named after Vilfredo Pareto.

- https://en.wikipedia.org/wiki/Pareto_efficiency
- https://en.wikipedia.org/wiki/Multi-objective_optimization

After looking around some more, I'm starting to think that maybe I should suck it up and start reading some proper books on game theory. For reference:

- https://math.stackexchange.com/questions/3779545/books-for-game-theory
- https://www.amazon.co.jp/-/en/Martin-J-Osborne/dp/0195322487
- https://www.amazon.co.jp/dp/0262650401

Before I get ahead of myself - yes, I should probably read some books, but I shouldn't fall into the "theory before practice" trap. I can read books _and_ work on my model as needed. The point is to have fun, not to optimize for maximum research efficiency or anything like that.

So, back to the model. I know that I need to optimize for two sets of things simultaneously - the wealth of each individual, and the wealth of the org as a whole. I can possibly expand this later to define the Org as a set of specific individuals, but that is getting way out of the scope I can currently handle, so I"m going to intentionally ignore it.

...Or _should_ I ignore it? Think back to what I originally wanted to prove. My whole problem is about shitty team leadership. I want to show that Orgs perform best when leaders consider each individual's needs on the same level as their (the Org's) needs. 

`(d = 01:06)`
---

I'm super tired...

Thinking about this a bit more, I think it should be OK to treat the Org as a separate entity after all. Even if I did try to implement Org leaders as a type of Individual, I'd still have to modify all the current logic in order to support that, which would be yet more "theory before practice" stuff.

So let's just define it like this. All Individuals _and_ the Org have a _wealth_ value. Now this is where I have to make a very important choice. The Org can either redistribute its wealth to all individuals, or it could consume it for itself. Of course, in practice it's going to be a mix of these two, but the specific way this mix is constructed seems like a rather philosophical question.


Judging by my 27-year old understanding the world, I suppose for-profit companies' purpose is to maximize value for shareholders, and, more often than not, that value is not particularly redistributed to employees (at least not in any specific and targeted way other than the assumption that wealth moving out of the company is "diffused" back into society). So for starters, I could assume that wealth generated by the Org is generally consumed by the Org, with the exception of wealth that is reinvested. Now that I think about it, this actually aligns pretty well with the accounting view of corporations. Money given back to shareholders as dividends essentially disappears from the system. As well.

I could have a meter for total wealth consumed by the Org (the total shareholder value generated). That takes care of the Org side of things. But then what about Individuals?

Individuals regularly consume a certain amount of wealth in order to survive. But their goal is not necessarily the accumulation of wealth itself. I suppose it's non-trivial to define a specific measure of "value" for Individuals, but I could borrow on some psychological ideas and define Individual "value" as the desire to grow and become better. One possible implementation of this is as a quanitty which can be increased by spending a certain amount of wealth (this is also similar to how some video games work I suppose). I could even add mechanices for diminishing returns and whatnot.

Ok, I think I can work with these definitions. I want to make a few different models and compare them. The comparison criteria would be as follows:

1. The total amount of Org Value that the Org has generated.
2. The min/avg/median/max of the amount of Individual Value for each Individual.
3. The total amount of Individual Value produced in the entire world.

Additionally, I should set a specific timeframe to be used in all experiments. It should be something realistic. One option would be something like 50 years - the total amount of time someone might work in a given Org. Then again, this rests on the assumption that Individuals work for the same Org during the entire time... But I can counter this by just implementing strategies that don't necessarily depend on how long someone's been in the company. Or by giving Individuals the option to leave voluntarily (but then, what's a good reason to leave? Median income based on age or something?)

The more I think about this, the more complicated it becomes...

I want to design abstract rules for the World, but I can't design abstractions for things which I don't know yet... I haven't even implemented the above setup yet, but I'm already trying to think of ways that it's not abstract enough (e.g. how would I handle non-profits?). I should probably lower my abstraction goals for this. It kinda sucks, since I just finished designing something somewhat abstract. But I really need to make some concrete things first and _then_ move on to abstractions.

Another thing I kinda want to think about is how the Org and Individuals are going to interact with each other. Up until now I kinda assumed a pretty simplistic turn-based model, but there are probably loads of different ways I could implement this. For starters, in the real world everything happens simultaneously, so that kind of approach could be pretty realistic. But then again simultaneous actions would be pretty hard to implement. I guess this is where the concept of games with complete/incomplete information come up.

Let's try to think this through. In a normal company, people don't usually act simultaneously against each other. In fact, most people's actions are usually constrained to their immediate surroundings, and rarely impact other people directly. Of course, say pair programming is a very good counter-example to this, but I think it's okay to ignore that kind of scenario for now.

Back to the main topic. Assuming a decent enough Org, I should be able to model Individuals' actions like this:

1. Do work - don't spend any individual wealth, generate some Org wealth
2. Invest personal wealth - spend individual wealth to increase one's own score, do not generate Org wealth

Actually, if I think aobut this seriously, just "wealth" might not be sufficient enough to express complex Individual behaviour. For starters, I could consider "time" as well. Maybe even "interest". But those are qualities I can implement later on I guess. For now I should keep my model simple, so that I could get specific results faster.

The other thing I must consider is the behaviour of the Org. Under the current rules, its objective is to maximize Org Value, which is counteracted by the need to provide wealth to Individuals which generate wealth to the Org. So the Org has to make a decision on how much wealth to distribute to each Individual, versus how much wealth to invest into itself.

Yet another thing to consider is how the organization recruits/fires people. Considering I live in Japan, firing is kinda hard. As a start, I could just ignore the concept and try to emulate it by setting the Individual's salary really low. On the other hand, recruitment is necessary. I'm tempted to think that the current recruitment implementation I have would be decent enough - the Org evaluates all employees, then the world generates some candidates, then the Org recruits a number of those candidates. Each canidate provides a set of "public" attributes which are visible by the Org, then the Org makes its decision based on that.

Ok that's quite a bit of thinking. I'll stop for now and go cook dinner. After that I could try to implement some of the ideas here and see how it goes.

`(d = 01:08)`
---

Again, I'm starting to get ahead of myself. I still haven't defined a proper goal for what I'm trying to do. Going back to what I wrote previously:

> I want to make a few different models and compare them. The comparison criteria would be as follows:
> 1. The total amount of Org Value that the Org has generated.
> 2. The min/avg/median/max of the amount of Individual Value for each Individual.
> 3. The total amount of Individual Value produced in the entire world.

This is good and all, but "make a few different models" is still too vague. How many models? What would the difference be? What are the common parts?

I'm starting to think I'm experiencing a chicken and egg problem here. I want to play around with a game that I haven't even made, yet the criteria for making it are somewhat dependent on what I want to play around with. On top of that, I'm trying to think of this in terms of a game _framework_ which is even harder to reason about.

Maybe I should just bite the bullet and give up on the framework part of it? Sure, it sonuds nice having a framework and all, but at the end of the day my goal is not to make it easy to build lots of games (though that might eventually manifest as a convenient _method_ of achieving my real goals). Maybe I should just decide on a set of rules, call that v1 and start working towards implementing all of them. I can worry about extensibility and whatnot later on. Further yet, now that I have the versioning problem figured out, it shouldn't be too hard to maintain multiple versions of multiple games running around.

Ok screw it let's just start hacking on a v1.

